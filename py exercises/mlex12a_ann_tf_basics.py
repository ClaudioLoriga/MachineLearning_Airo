# -*- coding: utf-8 -*-
"""MLEx12a.ANN_tf_basics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FuvDyO3nQ3aOWfLUfsLNcEnKhOSvLSIt

# Machine Learning Exercise

# 12a. ANN and Tensorflow basics

See https://www.tensorflow.org/guide/basics for details
"""

import tensorflow as tf

# Tensors are immutable
t = tf.constant([0.1, 0.4, 0.7], dtype=tf.float32)
print(t)
t = t * 2
print(t)

# Variables are mutable
v = tf.Variable([0.1, 0.4, 0.7], dtype=tf.float32)
print(v)
v.assign([0.2, 0.8, 1.4])
print(v)

"""# Automatic differentiation

## One variable
"""

def f(x): # f
  return x**3 - 2*x**2 + 3*x + 1

def f1(x): # analytical f'
  return 3*x**2 - 4*x + 3

def f1auto(x):  # auto f'
  x = tf.Variable(x, dtype=tf.float32)
  with tf.GradientTape() as tape:
    y = f(x)
  return tape.gradient(y, x)

for x in [0.0, 1.0, 2.0, 3.0]:
  print(f"x: {x} f': {f1(x)} f'auto {f1auto(x)}")

""" ## Two variables

Function f(x,w)

Derivative df/dx
"""

def f(x,w): # f
  return w[3] * x**3 + w[2] * x**2 + w[1] * x + w[0]

def f1(x,w): # analytical f' = df/dx
  return 3 * w[3] * x**2 + 2 * w[2] * x + w[1]

def f1auto_x(x,w):  # auto f' = df/dx
  x = tf.Variable(x, dtype=tf.float32)
  with tf.GradientTape() as tape:
    y = f(x,w)
  return tape.gradient(y, x)

w = tf.Variable([1, 3, -2, 1], dtype=tf.float32)
for x in [0.0, 1.0, 2.0, 3.0]:
  print(f"x: {x} f': {f1(x,w)} f'auto {f1auto_x(x,w)}")

"""## Gradient of loss function

Gradient of a loss function with respect to the parameters of a model
"""

# model
def f(x,w):
  return w[3] * x**3 + w[2] * x**2 + w[1] * x + w[0]

# loss function
def loss_fn(y,t):
  return (y-t)**2

# autodiffe dL/dw
def grad_loss_w(x,w,t):  # auto dL/dw
  x = tf.Variable(x, dtype=tf.float32)
  w = tf.Variable(w, dtype=tf.float32)
  with tf.GradientTape() as tape:
    y = f(x,w)
    loss = loss_fn(y,t)
  return tape.gradient(loss, w)

x = 1
t = 1
w = tf.Variable([1, 3, -2, 1], dtype=tf.float32)
y = f(x,w)
loss = loss_fn(y,t)

print(f"w: {w}")
print(f"x: {x} t: {t} y: {f(x,w):.3f} loss: {loss:.3f}")

grad_w = grad_loss_w(x,w,t)

print(f"grad of loss wrt w: {grad_w}")

"""One step of training"""

eta = 0.01

grad_w = grad_loss_w(x,w,t)
w = w - eta * grad_w
y = f(x,w)
loss = loss_fn(y,t)

print(f"w: {w}")
print(f"x: {x} t: {t} y: {f(x,w):.3f} loss: {loss:.3f}")

"""# Simple neural network

2 hidden units: ReLU

1 output unit: Linear

"""

# simple two-layers NN
def nn(x,w):
  h1 = tf.nn.relu(w[0] * x[0] + w[1]* x[1] + w[2])
  h2 = tf.nn.relu(w[3] * x[0] + w[4]* x[1] + w[5])
  y = w[6] * h1 + w[7] * h2 + w[8]
  return y

# loss function
def loss_fn(y,t):
  return (y-t)**2

# auto dl/dw
def grad_loss_w(x,w,t):
  with tf.GradientTape() as tape:
    tape.watch(w) # needed to return the gradients
    y = nn(x,w)
    loss = loss_fn(y,t)
  return tape.gradient(loss, w)

# one sample (x,t) in the dataset
x = tf.Variable([0, 1], dtype=tf.float32)
t = 1.0

# initial model parameters
w = tf.Variable([1, 3, -2, 1, 3, -2, 1, 1, 0], dtype=tf.float32)


# initial loss
y = nn(x, w)
loss = loss_fn(y, t)

print("Initial loss")
print(f"w0: {w.numpy()}")
print(f"x: {x.numpy()} y: {y} loss: {loss:.3f}")

# gradient descent

print("Gradient descent")
eta = 0.03  # learning rate

for i in range(10):
  # compute gradients
  grad_w = grad_loss_w(x,w,t)
  # update weights using gradients
  w = w - eta * grad_w
  # compute loss
  y = nn(x, w)
  loss = loss_fn(y, t)
  print(f"  {i} x: {x.numpy()} y: {y:.3f} loss: {loss:.3f}")

print(f"final value w*: {w.numpy()}  final loss: {loss:.3f}")