# -*- coding: utf-8 -*-
"""MLEx12b.ANN_torch_basics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YdhChMxG0TQRQxMdlsK1gjWcyHx0U-CA

# Machine Learning Exercise

# 12b. ANN and PyTorch basics

See https://docs.pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html for details
"""

import torch

# Tensors are immutable
t = torch.tensor([0.1, 0.4, 0.7], dtype=torch.float32)
print(t)
t = t * 2
print(t)

# Tensors with grad

x = torch.tensor([0.1, 0.4, 0.7], dtype=torch.float32, requires_grad=True)
print(f"x = {x}")
y = 0.5 * x.pow(2).sum() # y = 1/2 sum_i x_i^2
print(f"y = {y}")
y.backward()
print(f"dy/dx = {x.grad}")

"""# Automatic differentiation

## One variable
"""

def f(x): # f
  return x**3 - 2*x**2 + 3*x + 1

def f1(x): # analytical f' = dy/dx
  return 3*x**2 - 4*x + 3

def f1auto(x):  # auto f' = dy/dx
  x = torch.tensor(x, dtype=torch.float32, requires_grad=True)
  y = f(x)
  y.backward()
  return x.grad

for x in [0.0, 1.0, 2.0, 3.0]:
  print(f"x: {x} f': {f1(x)} f'auto: {f1auto(x)}")

"""## Two variables

Function f(x,w)

Derivative df/dx
"""

def f(x,w): # f
  return w[3] * x**3 + w[2] * x**2 + w[1] * x + w[0]

def f1(x,w): # analytical f' = df/dx
  return 3 * w[3] * x**2 + 2 * w[2] * x + w[1]

def f1auto_x(x,w):  # auto f' = df/dx
  x = torch.tensor(x, dtype=torch.float32, requires_grad=True)
  y = f(x,w)
  y.backward()
  return x.grad

w = torch.tensor([1, 3, -2, 1], dtype=torch.float32)
for x in [0.0, 1.0, 2.0, 3.0]:
  print(f"x: {x} f': {f1(x,w)} f'auto: {f1auto_x(x,w)}")

"""## Gradient of loss function

Gradient of a loss function with respect to the parameters of a model
"""

# model
def f(x,w):
  return w[3] * x**3 + w[2] * x**2 + w[1] * x + w[0]

# loss function
def loss_fn(y,t):
  return (y-t)**2

# autodiffe dL/dw
def grad_loss_w(x,w,t):  # auto dL/dw
  x = torch.tensor(x, dtype=torch.float32, requires_grad=True)
  y = f(x,w)
  loss = loss_fn(y,t)
  if w.grad is not None:
    w.grad.zero_()  # reset grad before backward step
  loss.backward()
  return w.grad

x = 1
t = 1
w = torch.tensor([1, 3, -2, 1], dtype=torch.float32, requires_grad=True)
y = f(x,w)
loss = loss_fn(y,t)

print(f"w: {w}")
print(f"x: {x} t: {t} y: {f(x,w):.3f} loss: {loss:.3f}")

grad_w = grad_loss_w(x,w,t)

print(f"grad of loss wrt w: {grad_w}")

"""One step of training"""

eta = 0.01

grad_w = grad_loss_w(x,w,t)
with torch.no_grad():
  w -= eta * grad_w
y = f(x,w)
loss = loss_fn(y,t)

print(f"w: {w}")
print(f"x: {x} t: {t} y: {f(x,w):.3f} loss: {loss:.3f}")

"""# Simple neural network

2 hidden units: ReLU

1 output unit: Linear

"""

from torch.nn.functional import relu

# simple two-layers NN
def nn(x,w):
  h1 = relu(w[0] * x[0] + w[1]* x[1] + w[2])
  h2 = relu(w[3] * x[0] + w[4]* x[1] + w[5])
  y = w[6] * h1 + w[7] * h2 + w[8]
  return y

# loss function
def loss_fn(y,t):
  return (y-t)**2

# auto dL/dw
def grad_loss_w(x,w,t):
  # Zero out gradients before each iteration
  if w.grad is not None:
      w.grad.zero_()
  # forward computation
  y = nn(x, w)
  loss = loss_fn(y, t)
  # Calculate gradients
  loss.backward()
  return w.grad

# one sample (x,t) in the dataset
x = torch.tensor([0, 1], dtype=torch.float32)
t = torch.tensor(1, dtype=torch.float32)

# initial model parameters
w = torch.tensor([1, 3, -2, 1, 3, -2, 1, 1, 0], dtype=torch.float32, requires_grad=True)

# initial loss
y = nn(x,w)
loss = loss_fn(y,t)

print("Initial loss")
print(f"w0: {w}")
print(f"x: {x.numpy()} t: {t.item():.1} y: {y.item():.3f} loss: {loss.item():.3f}")


# gradient descent

print("Gradient descent")
eta = 0.03  # learning rate

for i in range(10):

  # compute gradients
  grad_w = grad_loss_w(x,w,t)

  # update weights using gradients
  with torch.no_grad():
    w -= eta * grad_w

  # compute loss
  y = nn(x,w)
  loss = loss_fn(y,t)

  print(f"  {i} x: {x.numpy()} t: {t.item():.1f} y: {y.item():.3f} loss: {loss.item():.3f}")

print(f"final value w*: {w}    final loss: {loss:.3f}")