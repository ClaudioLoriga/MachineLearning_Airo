# -*- coding: utf-8 -*-
"""MLEx13a.LeNet

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qDn5Al7xLPY2eQ-cbcUZwt1thwuUSZBJ

# Machine Learning Exercise

# 13a. LeNet-like model for MNIST/FashionMNIST/CIFAR10

##Import

Import libraries and print some versions.

To use GPU, set `Edit / Notebook settings / Hardware accelerator` to **GPU**.
"""

import numpy as np
import tensorflow as tf

print("Tensorflow version %s" %tf.__version__)

device_name = tf.test.gpu_device_name()
print('Found GPU at: {}'.format(device_name))

"""# Choose experiment configuration"""

dataset_name = "MNIST" # MNIST or FASHION_MNIST or CIFAR10
seed = 101
learning_rate = 0.001

use_wandb = False  # if you want to monitor results with W&B

tf.keras.utils.set_random_seed(seed)

"""## Load data

Load training data from Keras library



"""

def load_data(dataset_name):
    # load data
    if dataset_name == "MNIST":
      (Xtrain,Ytrain), (Xtest, Ytest) = tf.keras.datasets.mnist.load_data()
    elif dataset_name == "FASHION_MNIST":
      (Xtrain,Ytrain), (Xtest, Ytest) = tf.keras.datasets.fashion_mnist.load_data()
    elif dataset_name == "CIFAR10":
      (Xtrain,Ytrain), (Xtest, Ytest) = tf.keras.datasets.cifar10.load_data()
    else:
      print("Dataset not available")
      return None

    # get information
    ninput = Xtrain.shape[0]
    imgsize = (Xtrain.shape[1], Xtrain.shape[2])
    if len(Xtrain.shape)==3:
      image_depth=1
    else:
      image_depth=Xtrain.shape[3]

    input_shape = (Xtrain.shape[1], Xtrain.shape[2], image_depth)
    ntest = Xtest.shape[0]
    if len(Ytrain.shape)>1:  # datasets output have different shapes
      num_classes = np.max(Ytrain[:,0]) + 1
    else:
      num_classes = np.max(Ytrain) + 1

    print("Training input %s" %str(Xtrain.shape))
    print("Training output %s" %str(Ytrain.shape))
    print("Test input %s" %str(Xtest.shape))
    print("Test output %s" %str(Ytest.shape))
    print("Input shape: %s" %str(input_shape))
    print("Number of classes: %d" %num_classes)

    # normalize input to [0,1]
    Xtrain = Xtrain / 255.0
    Xtest = Xtest / 255.0
    # reshape input in 4D array
    Xtrain = Xtrain.reshape(ninput,imgsize[0],imgsize[1],image_depth)
    Xtest = Xtest.reshape(ntest,imgsize[0],imgsize[1],image_depth)

    return [Xtrain,Ytrain,Xtest,Ytest,input_shape,num_classes]



[Xtrain,Ytrain,Xtest,Ytest,input_shape,num_classes] = load_data(dataset_name)

"""## Show random image


"""

import matplotlib.pyplot as plt
import random

i = random.randrange(0,Xtrain.shape[0])
image = Xtrain[i]
image = np.array(image, dtype='float')
pixels = image.reshape((input_shape[0], input_shape[1], input_shape[2]))

label = Ytrain[i]
print(label)

plt.imshow(pixels)
plt.show()

"""##LeNet model

"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Dense, Activation, Dropout, Flatten,\
                         Conv2D, MaxPooling2D, AveragePooling2D, BatchNormalization
from tensorflow.keras import regularizers, optimizers

def LeNet(input_shape, num_classes, lr):

    print('\nLeNet model')
    model = Sequential()

    model.add(Input(shape=input_shape))
    print('\tC1: Convolutional 6 kernels 5x5')
    model.add(Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='same'))
    print('\tS2: Average Pooling 2x2 stride 2x2')
    model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))
    print('\tC3: Convolutional 16 kernels 5x5')
    model.add(Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid'))
    print('\tS4: Average Pooling 2x2 stride 2x2')
    model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))
    print('\tC5: Convolutional 120 kernels 5x5')
    model.add(Conv2D(120, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid'))
    model.add(Flatten())
    print('\tF6: Fully connected, 84 units')
    model.add(Dense(84, activation='tanh'))
    print('\tF7: Fully connected, 10 units')
    model.add(Dense(num_classes, activation='softmax'))

    optimizer = optimizers.Adam(learning_rate=lr)  # try other optimizers and hyper-parameters

    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])

    return model

# create the model

model = LeNet(input_shape,num_classes, learning_rate)
model.summary()

"""# W&B monitor

Run this cell if you want to monitor results with [W&B](https://wandb.ai).

You need an account and the token generated after log in to proceed.
"""

if use_wandb:
  !pip install wandb -qU

  import wandb
  from wandb.integration.keras import WandbMetricsLogger
  wandb.login()

  wandb.init(
    # Set the project where this run will be logged
    project="LeNet",
    # Experiment name
    name = f"LeNet_{dataset_name}_{seed}",
    # Track hyperparameters and run metadata
    config={
        "architecture": "LeNet",
        "learning_rate": learning_rate,
        "dataset": dataset_name,
        "seed": seed,
    },
)

"""## Train"""

epochs = 30

cbks = []
if use_wandb:
  cbks.append(WandbMetricsLogger())

history = model.fit(Xtrain, Ytrain, batch_size=32, epochs=epochs, validation_data = (Xtest,Ytest),
                    callbacks=cbks )

"""## W&B monitor closing

Close when the experiment is terminated.
"""

if use_wandb:
  wandb.finish()

"""##Evaluate the model

## Print scores

Overall accuracy, precision, recall and F-score
"""

import sklearn.metrics
from sklearn.metrics import classification_report, confusion_matrix

# accuracy
score = model.evaluate(Xtest, Ytest)
print("Test loss: %f" %score[0])
print("Test accuracy: %f" %score[1])

preds = model.predict(Xtest,verbose=1)
Ypred = np.argmax(preds, axis=1)

print('%s' %str(Ypred.shape))
print('%s' %str(Ytest.shape))

print(classification_report(Ytest, Ypred, digits=3))

"""##Plot results"""

import matplotlib.pyplot as plt

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.ylim(np.min(history.history['accuracy']), 1.0)
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.ylim(0.0, np.max(history.history['loss']))
plt.legend(['train', 'test'], loc='lower left')
plt.show()

"""# Home Exercises

**Question 1**

Compare the model defined above with a model with the same structure but with `ReLU` activation functions in all the layers.

**Question 2**

Try other optimizers and parform some hyper-parameter search.



"""