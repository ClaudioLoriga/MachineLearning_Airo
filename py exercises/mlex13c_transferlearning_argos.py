# -*- coding: utf-8 -*-
"""MLEx13c.TransferLearning_ARGOS

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i2Madm_IQfQBNvVpKj8KbKMXbnYUqoqq

# Machine Learning Exercise

# 13c. Transfer Learning - ARGOS

This exercise shows how to apply transfer learning from ImageNet to ARGOS domain.

##Import

Import libraries and print some versions.

To use GPU, set `Edit / Notebook settings / Hardware accelerator` to **GPU**.
"""

import numpy as np
import tensorflow as tf

print("Tensorflow version %s" %tf.__version__)

device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

"""## Load data

Get data from
https://drive.google.com/drive/folders/0Bxxqx_AAp2u2Zkp4cGxoNVEzb3M?resourcekey=0-RKOYOxuHZTxh0eSZRSnIoA

Place data in a local folder or a Google folder and set the datadir variable below with the path of this folder.

Run the docker image mounting the data folder or get Google Drive access

Load training data

4774 images from 20 classes


"""

import os

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Input, Dense, Activation, Dropout, Flatten,\
                         Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D,\
                         UpSampling2D, BatchNormalization
from tensorflow.keras import regularizers
from tensorflow.keras import optimizers
from tensorflow.keras import callbacks

import matplotlib.pyplot as plt
import pickle

use_local_data = False

if use_local_data:
    datadir = '/opt/Data/ARGOS_public'  # folder in docker container with dataset
else:
    from google.colab import drive
    drive.mount('/content/drive')
    #datadir = '/content/drive/My Drive/ARGOS_public'
    datadir = '/content/drive/My Drive/Data/ARGOS-Data/ARGOS_public'

trainingset = datadir+'/train/'
testset = datadir + '/test/'
models_dir = datadir + '/models/'
results_dir = datadir + '/results/'

def savemodel(model,problem):
    filename = os.path.join(models_dir, '%s.h5' %problem)
    model.save(filename)
    print("\nModel saved on file %s\n" %filename)

def savehistory(history,problem):
    filename = os.path.join(results_dir, '%s.hist' %problem)
    with open(filename, 'wb') as f:
        pickle.dump(history.history, f, pickle.HIGHEST_PROTOCOL)
    print("\nHystory saved on file %s\n" %filename)

def loadhistory(problem):
    filename = os.path.join(results_dir, '%s.hist' %problem)
    with open(filename, 'rb') as f:
        history = pickle.load(f)
    print("\nHystory loaded from file %s\n" %filename)
    return history

"""Some parameters of the training process"""

image_size = (118, 224)
input_shape = (image_size[0], image_size[1], 3)

batch_size = 32

data_augmentation_level = 1

"""## Dataset object with image dataset iterator"""

from tensorflow.keras.layers import RandomFlip,RandomRotation,RandomZoom,RandomContrast,Rescaling,RandomTranslation

# image flow of training samples from directory (without augmentation)
train_ds = tf.keras.utils.image_dataset_from_directory(
  trainingset,
  image_size=image_size,
  label_mode='categorical',
  batch_size=batch_size,
  shuffle=True)

classnames = train_ds.class_names
num_classes = len(classnames)

print("Image input %s" %str(input_shape))
print("Classes: %r" %classnames)
# re-scale images to 0-1 range
train_ds = train_ds.map(lambda x, y: (x/255.0, y))
#train_ds = train_ds.repeat(1)

if data_augmentation_level==0:
  pass
elif data_augmentation_level==1:
  # Create a preprocessing layer pipeline
  data_augmentation = tf.keras.Sequential([
      RandomRotation(0.05),
      RandomZoom(0.05),
  ])
  train_ds = train_ds.map(lambda x, y: (data_augmentation(x), y))
else:
  # Create a preprocessing layer pipeline
  data_augmentation = tf.keras.Sequential([
      RandomFlip("horizontal"),
      RandomRotation(0.05),
      RandomZoom(0.05),
      RandomTranslation(0.05,0.05)
  ])
  train_ds = train_ds.map(lambda x, y: (data_augmentation(x), y))


test_ds = tf.keras.utils.image_dataset_from_directory(
  testset,
  image_size=image_size,
  label_mode='categorical',
  batch_size=batch_size,
  shuffle=False)



# re-scale images to 0-1 range
test_ds = test_ds.map(lambda x, y: (x/255.0, y))
#test_ds = test_ds.repeat(1)

image_iterator = train_ds.as_numpy_iterator()

steps_per_epoch = len(train_ds)
val_steps = len(test_ds)

"""## Show random image


"""

n = 3
x,y = next(image_iterator)

for i in range(0,n):
    image = tf.keras.utils.array_to_img(x[i])
    label = y[i].argmax()  # categorical from one-hot-encoding
    print(classnames[label])
    plt.imshow(image)
    plt.show()

"""## Load base VGG16 model

Load VGG16 model pre-trained with ImageNet
"""

# Load VGG16 model
# init_weights='imagenet' if you want to use the pretrained model
def load_vgg16_features(input_shape, init_weights=None):

    # define input tensor
    input0 = Input(shape=input_shape)

    # load VGG16 model (possibly pretrained model on imagenet) without the final dense layers (include_top=False)
    vgg16_model = tf.keras.applications.vgg16.VGG16(include_top=False, weights=init_weights, input_tensor=input0)

    feature_extractor = tf.keras.models.Model(inputs=input0, outputs=vgg16_model.output, name="vgg16_features")

    adam = optimizers.Adam(learning_rate=1e-3)

    feature_extractor.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=adam, metrics=['accuracy'])

    return feature_extractor

vgg16feat_model = load_vgg16_features(input_shape)
vgg16feat_model.summary()

"""##VGG16-TransferNet

Transfer learning from VGG16 trained on ImageNet
"""

def transferNet(input_shape, num_classes, output_layer_name, trainable_layers):

    # load the pre-trained model
    feature_extractor = tf.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_shape=input_shape)

    # set the feture extractor layers as non-trainable
    for layer in feature_extractor.layers:
      if layer.name in trainable_layers:
        layer.trainable = True
      else:
        layer.trainable = False

    # get the output tensor from a layer of the feature extractor
    x = feature_extractor.get_layer(name = output_layer_name).output

    # flat the output of a Conv layer
    x = Flatten()(x)
    x = BatchNormalization()(x)

    # add a Dense layer
    x = Dropout(0.4)(x)
    x = Dense(200, activation='relu')(x)
    x = BatchNormalization()(x)

    # add a Dense layer
    x = Dropout(0.4)(x)
    x = Dense(100, activation='relu')(x)
    x = BatchNormalization()(x)

    # add the final output layer
    x = BatchNormalization()(x)
    predictions = Dense(num_classes, activation='softmax')(x)

    model = tf.keras.models.Model(inputs=feature_extractor.input, outputs=predictions, name="transferNet")

    adam = optimizers.Adam(learning_rate=1e-3)
    model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=adam, metrics=['accuracy'])

    return model



# choose the layer from which you can get the features (block5_pool the end, glob_pooling to get the pooled version of the output)
name_output_extractor = "block5_pool"
trainable_layers = ["block5_conv3"]

# build the transfer model
transfer_model = transferNet(input_shape, num_classes, name_output_extractor, trainable_layers)
transfer_model.summary()

"""### Train VGG16-ARGOS"""

# fit the transferNet on the training data
stopping = callbacks.EarlyStopping(monitor='val_accuracy', patience=3)


epochs = 10
try:
    transfer_history = transfer_model.fit(train_ds, epochs=epochs, verbose=1, callbacks=[stopping],\
                    steps_per_epoch=steps_per_epoch,\
                    validation_data=test_ds,\
                    validation_steps=val_steps)
except KeyboardInterrupt:
    pass

# Create directories if they don't exist
if not use_local_data:
  if not os.path.exists(models_dir):
      os.makedirs(models_dir)
  if not os.path.exists(results_dir):
      os.makedirs(results_dir)

"""Save the model and the history of results"""

# Save trained model and hystory
problemname = 'VGG16-transfer-ARGOS-da1-10ep_NEW'
savemodel(transfer_model,problemname)
savehistory(transfer_history,problemname)

"""## VGG16-ShallowNet

Use VGG16 features trained on ImageNet as input for a simple classifier based on Shallow NN
"""

# define a shallow neural net
def shallowNet(input_shape, num_classes):

    # load the pre-trained model
    feature_extractor = load_vgg16_features(input_shape, init_weights='imagenet')

    # set the feture extractor layers as non-trainable
    for idx,layer in enumerate(feature_extractor.layers):
        layer.trainable = False

    # get the last tensor from the feature extractor
    x = feature_extractor.output

    x = Flatten()(x)
    x = BatchNormalization()(x)

    # add the final output layer
    predictions = Dense(num_classes, activation='softmax')(x)


    shallow_model = Model(inputs=feature_extractor.input, outputs=predictions, name="shallowNet")


    adam = optimizers.Adam(learning_rate=1e-3)
    shallow_model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=adam, metrics=['accuracy'])

    return shallow_model


shallow_model = shallowNet(input_shape, num_classes)
shallow_model.summary()

"""### Train ShallowNet"""

# train the shallow model
stopping = callbacks.EarlyStopping(monitor='val_accuracy', patience=3)

epochs = 10
try:
    shallow_history = shallow_model.fit(train_ds, epochs=epochs, verbose=1, callbacks=[stopping],\
                    steps_per_epoch=steps_per_epoch,\
                    validation_data=test_ds,\
                    validation_steps=val_steps)
except KeyboardInterrupt:
    pass

if not use_local_data:
  if not os.path.exists(models_dir):
      os.makedirs(models_dir)
  if not os.path.exists(results_dir):
      os.makedirs(results_dir)

"""Save the model and the history of results"""

# Save trained model and hystory
problemname = 'VGG16-features-ShallowNN-ARGOS-da1-10ep_NEW'
savemodel(shallow_model,problemname)
savehistory(shallow_history,problemname)

"""## VGG16-SVM

Use VGG16 features to train an SVM model.
"""

# Load VGG16 pre-trained on Image Net
vgg16feat_model = load_vgg16_features(input_shape, init_weights='imagenet')

"""Normalize data for SVM"""

train_flow = tf.keras.preprocessing.image_dataset_from_directory(
  trainingset,
  image_size=image_size,
  label_mode='categorical',
  batch_size=batch_size,
  shuffle=False)  # for shuffle, you need to adjust the procedure to compute y_train below

test_flow = tf.keras.preprocessing.image_dataset_from_directory(
  testset,
  image_size=image_size,
  label_mode='categorical',
  batch_size=batch_size,
  shuffle=False)

# extract training features
x_feat = vgg16feat_model.predict(train_flow)

# global average pool on training features
x_feat_resh = np.mean(x_feat, axis=(1,2))
#x_feat_resh = np.reshape(x_feat, (x_feat.shape[0],-1))

# extract testing features
x_feat_test = vgg16feat_model.predict(test_flow)

# global average pool on testing features
#x_feat_test_resh = np.reshape(x_feat_test, (x_feat_test.shape[0],-1))
x_feat_test_resh = np.mean(x_feat_test, axis=(1,2))

# get the training/testing labels (sklearn format)


y_train = [y_b for _, y_b in train_flow]
y_test = [y_b for _, y_b in test_flow]

y_train = np.concatenate(y_train, axis=0)
y_test = np.concatenate(y_test, axis=0)

y_train = np.argmax(y_train, axis=-1)
y_test = np.argmax(y_test, axis=-1)

"""Train and evaluate SVM model"""

from sklearn import svm

kernel_name  = 'linear' # 'linear', 'poly', 'rbf', 'sigmoid'
C = 1

classifier = svm.SVC(C=C, kernel=kernel_name, degree=3, gamma="scale")
classifier.fit(x_feat_resh,y_train)

svm_acc = classifier.score(x_feat_test_resh, y_test)
print("Accuracy %.3f" %svm_acc)

"""##Plot results"""

import matplotlib.pyplot as plt

def plot_results(histories,legend):

    plt.title('Test accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.ylim(0.7,1.0)
    for h in histories:
        plt.plot(h)
    plt.legend(legend, loc='upper left')
    plt.show()



transfer_history = loadhistory('VGG16-transfer-ARGOS-10ep_NEW')
shallow_history = loadhistory('VGG16-features-ShallowNN-ARGOS-10ep_NEW')
svm_hist = [svm_acc for _ in range(len(shallow_history['val_accuracy']))]


legend = ["TransferNet", "ShallowNet", "SVM"]
histories = [transfer_history['val_accuracy'], shallow_history['val_accuracy'], svm_hist]
plot_results(histories,legend)