# -*- coding: utf-8 -*-
"""MLEx8a.LinearRegression

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yDZ1NyaFPwtWGHi-krO8flgYSM-L8B6x

# Machine Learning Exercise
# 8a. Linear/kernel Regression

Example adapted from
https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py

## Import libraries and load data set

Using [Diabetes dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset)

For visualization purposes, data set is reduced to 1 dimension.
"""

import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.svm import SVR


# Load the diabetes dataset
diabetes = datasets.load_diabetes()

print('Dataset: original-diabetes')
print('Number of features: %d' %diabetes.data.shape[1])
print('Number of samples: %d' %diabetes.data.shape[0])

# Use only one feature
diabetes_X = diabetes.data[:, np.newaxis, 2]


# Split the data into training/testing sets
diabetes_X_train = diabetes_X[:-20]
diabetes_X_test = diabetes_X[-20:]

# Split the targets into training/testing sets
diabetes_y_train = diabetes.target[:-20]
diabetes_y_test = diabetes.target[-20:]

print('Dataset: diabetes-1D')
print('Number of features: %d' %diabetes_X.data.shape[1])
print('Number of samples: %d' %diabetes_X.data.shape[0])

print('Training set size: %d' %len(diabetes_X_train))
print('Test set size: %d' %len(diabetes_X_test))

print('Target range: [%.1f, %.1f]' %(min(diabetes.target),max(diabetes.target)))

"""## Least square"""

class LeastSquare:

    def __init__(self):
        self.w = [0, 0]

    def fit(self,X,t):
        n = len(X) # nr. of examples
        phi = np.c_[np.ones(n), X] # design matrix
        self.w = np.matmul(np.linalg.pinv(phi),t) # Least square solution
        print("Least square solution: %s" %(str(self.w.transpose())))

    def predict(self,x):
        n = len(x)
        phi = np.c_[np.ones(n), x] # design matrix
        yn = np.matmul(self.w.transpose(),phi.transpose())
        return yn

"""## Train regression model

*   Linear regression ([info](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html))
*   SVM with linear kernel ([info](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html))
*   SVM with polynomial kernel ([info](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html))

C: regularization factor, weight for the error of the model

Optimization function implemented in the library: C * E_model + E_reg

Note: in the slides we use E_model + λ E_reg

C inverse of λ


"""

model_type = "least_square"  # "least_square", "linear_regression", "linear_svm", "poly_svm"

if model_type == "least_square":
  # LeastSquare model
  model = LeastSquare()
  # Train the model using the training set
  model.fit(diabetes_X_train, diabetes_y_train)

elif model_type == "linear_regression":
  # Create linear regression object
  model = linear_model.LinearRegression()
  # Train the model using the training set
  model.fit(diabetes_X_train, diabetes_y_train)

elif model_type == "linear_svm":
  # SVM regression
  model = SVR(kernel='linear', C=1.0)
  # Train the model using the training set
  model.fit(diabetes_X_train, diabetes_y_train)

elif model_type == "poly_svm":
  # SVM polynomial regression
  model = SVR(kernel='poly', C=1.0, degree=3, gamma='scale')
  # Train the model using the training set
  model.fit(diabetes_X_train, diabetes_y_train)

"""## Prediction"""

# Commented out IPython magic to ensure Python compatibility.
# Make predictions using the testing set
diabetes_y_pred = model.predict(diabetes_X_test)

# The mean squared error
print("Mean squared error: %.2f"
#       % mean_squared_error(diabetes_y_test, diabetes_y_pred))

# R2 regression score: 1 is perfect prediction
print('Regression score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))

"""## Plot results"""

# Plot outputs
plt.scatter(diabetes_X_test, diabetes_y_test,  color='black', label='Data')
plt.scatter(diabetes_X_test, diabetes_y_pred, color='red', linewidth=3, label='Prediction')
plt.legend()

plt.xticks(())
plt.yticks(())
plt.show()

"""# Generated dataset"""

def f(x): # true fn
  return x**3 - 2*x**2 + 3*x + 1

n = 250 # number of data points

x_train = np.linspace(-2.5, 2.5, n) # generate input samples
y_train = f(x_train) + np.random.normal(loc=0, scale=2, size=(n,))  # noisy output

x_test = np.linspace(-2.5, 2.5, int(n/5)) # generate input samples
y_test = f(x_test) + np.random.normal(loc=0, scale=2, size=(int(n/5),))  # noisy output

print("Dataset generated")

"""## Plot dataset"""

plt.plot(x_train, y_train, '.', label='Data')
plt.plot(x_train, f(x_train), label='True fn')
plt.legend()

"""## Train"""

# Commented out IPython magic to ensure Python compatibility.
# SVM polynomial regression
model = SVR(kernel='poly', C=1.0, degree=3, gamma='scale')
# Train the model using the training sets
model.fit(x_train.reshape(-1,1), y_train)

y_pred = model.predict(x_test.reshape(-1,1))
print("Mean squared error: %.2f"
#       % mean_squared_error(y_test, y_pred))

# R2 regression score: 1 is perfect prediction
print('Regression score: %.2f' % r2_score(y_test, y_pred))

"""## Plot results"""

plt.plot(x_train, y_train, '.', label='Data')
plt.plot(x_train, f(x_train), label='True fn')
plt.plot(x_train, model.predict(x_train.reshape(-1,1)), label='Prediction')
plt.legend()

"""# Home Exercises

Tune the SVM regressors on a held-out validation set.

**Question 1**

Consider the hyper-parameter C in the Linear SVM and plot the testing performance as the paramater varies.

**Question 2**

Consider the hyper-parameters C and degree of the polynomial in the Polynomial SVM. Perform a grid search and show the obtained results.

"""